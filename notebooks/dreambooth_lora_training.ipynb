{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b0a6d49-f7b6-431a-89b3-1b1d3a43326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['INSTANCE_DIR'] = os.path.abspath(os.path.join(os.pardir, \"jessica_alba\", \"instance_images\"))\n",
    "os.environ['CLASS_DIR'] = os.path.abspath(os.path.join(os.pardir, \"jessica_alba\", \"class_images\"))\n",
    "\n",
    "os.environ['MODEL_PATH'] = os.path.abspath(os.path.join(os.pardir, \"models\", \"diffusers_converted_model\"))\n",
    "os.environ['OUTPUT_DIR'] = os.path.abspath(os.path.join(os.pardir, \"models\", \"dreambooth_lora_32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dde53ead-93e1-449a-9b3a-46ceeddd84bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f05d8262-031e-4472-942b-0270b3ebc72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/19/2024 13:44:10 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'variance_type'} was not found in config. Values will be initialized to default values.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mildar-azamatov\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ildar.azamatov/projects/itmo/itmo-generative-models/notebooks/wandb/run-20240419_134413-d2mh203m\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdeep-elevator-9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ildar-azamatov/dreambooth-lora\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ildar-azamatov/dreambooth-lora/runs/d2mh203m\u001b[0m\n",
      "04/19/2024 13:44:14 - INFO - __main__ - ***** Running training *****\n",
      "04/19/2024 13:44:14 - INFO - __main__ -   Num examples = 500\n",
      "04/19/2024 13:44:14 - INFO - __main__ -   Num batches each epoch = 125\n",
      "04/19/2024 13:44:14 - INFO - __main__ -   Num Epochs = 7\n",
      "04/19/2024 13:44:14 - INFO - __main__ -   Instantaneous batch size per device = 4\n",
      "04/19/2024 13:44:14 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "04/19/2024 13:44:14 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "04/19/2024 13:44:14 - INFO - __main__ -   Total optimization steps = 800\n",
      "Steps: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [20:14<00:00,  1.52s/it, loss=0.177, lr=0.0002]04/19/2024 14:04:28 - INFO - accelerate.accelerator - Saving current state to /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/dreambooth_lora_32/checkpoint-800\n",
      "/home/ildar.azamatov/projects/itmo/itmo-generative-models/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/diffusers_converted_model - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "Model weights saved in /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/dreambooth_lora_32/checkpoint-800/pytorch_lora_weights.safetensors\n",
      "04/19/2024 14:04:28 - INFO - accelerate.checkpointing - Optimizer state saved in /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/dreambooth_lora_32/checkpoint-800/optimizer.bin\n",
      "04/19/2024 14:04:28 - INFO - accelerate.checkpointing - Scheduler state saved in /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/dreambooth_lora_32/checkpoint-800/scheduler.bin\n",
      "04/19/2024 14:04:28 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/dreambooth_lora_32/checkpoint-800/sampler.bin\n",
      "04/19/2024 14:04:28 - INFO - accelerate.checkpointing - Random states saved in /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/dreambooth_lora_32/checkpoint-800/random_states_0.pkl\n",
      "04/19/2024 14:04:28 - INFO - __main__ - Saved state to /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/dreambooth_lora_32/checkpoint-800\n",
      "Steps: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [20:14<00:00,  1.52s/it, loss=0.087, lr=0.0002]Model weights saved in /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/dreambooth_lora_32/pytorch_lora_weights.safetensors\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded scheduler as DDIMScheduler from `scheduler` subfolder of /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/diffusers_converted_model.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/diffusers_converted_model.\n",
      "\n",
      "Loading pipeline components...:  29%|‚ñà‚ñà‚ñà‚ñã         | 2/7 [00:01<00:02,  1.71it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/diffusers_converted_model.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/diffusers_converted_model.\n",
      "\n",
      "Loading pipeline components...:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 4/7 [00:01<00:00,  3.31it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/diffusers_converted_model.\n",
      "\n",
      "Loading pipeline components...:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/7 [00:01<00:00,  3.97it/s]\u001b[A/home/ildar.azamatov/projects/itmo/itmo-generative-models/.venv/lib/python3.10/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Loaded feature_extractor as CLIPFeatureExtractor from `feature_extractor` subfolder of /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/diffusers_converted_model.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/diffusers_converted_model.\n",
      "Loading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:01<00:00,  4.54it/s]\n",
      "Loading unet.\n",
      "Loading text_encoder.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: loss ‚ñÑ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñá‚ñÉ‚ñÜ‚ñÉ‚ñÇ‚ñÜ‚ñà‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñá‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÜ‚ñÉ‚ñÇ‚ñÉ‚ñÜ‚ñÉ‚ñÇ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   lr ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: loss 0.08696\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   lr 0.0002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mdeep-elevator-9\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ildar-azamatov/dreambooth-lora/runs/d2mh203m\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ô∏è‚ö° View job at \u001b[34m\u001b[4mhttps://wandb.ai/ildar-azamatov/dreambooth-lora/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2NDM2NjUyMg==/version_details/v2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240419_134413-d2mh203m/logs\u001b[0m\n",
      "/home/ildar.azamatov/projects/itmo/itmo-generative-models/.venv/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:2171: UserWarning: Run (d2mh203m) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
      "Steps: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [20:21<00:00,  1.53s/it, loss=0.087, lr=0.0002]\n"
     ]
    }
   ],
   "source": [
    "!python3 diffusers/examples/dreambooth/train_dreambooth_lora.py \\\n",
    "  --pretrained_model_name_or_path=$MODEL_PATH \\\n",
    "  --instance_data_dir=$INSTANCE_DIR \\\n",
    "  --class_data_dir=$CLASS_DIR \\\n",
    "  --output_dir=$OUTPUT_DIR \\\n",
    "  --instance_prompt=\"a photo of sks woman face\" \\\n",
    "  --class_prompt=\"a photo of woman\" \\\n",
    "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=4 \\\n",
    "  --learning_rate=2e-4 \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --num_class_images=500 \\\n",
    "  --max_train_steps=800 \\\n",
    "  --checkpointing_steps=800 \\\n",
    "  --use_8bit_adam \\\n",
    "  --mixed_precision=\"no\"\\\n",
    "  --train_text_encoder \\\n",
    "  --rank=32 \\\n",
    "  --report_to=\"wandb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c428bb44-9d7b-478a-8008-38c10050980f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900d98c9b9d6484983f69176b6ea17ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ildar.azamatov/projects/itmo/itmo-generative-models/.venv/lib/python3.10/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import autocast\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "model_path = os.environ['MODEL_PATH']\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_path, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.load_lora_weights(os.environ['OUTPUT_DIR'])\n",
    "g_cuda=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7665d0cd-cd02-4d91-94f4-5b877fde678b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe3605de990>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@markdown Can set random seed here for reproducibility.\n",
    "g_cuda = torch.Generator(device='cuda')\n",
    "seed = 345252 #@param {type:\"number\"}\n",
    "g_cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "230a3167-9474-41a5-99c3-e1439d6f5650",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"sks\"\n",
    "promt_list = [\n",
    "    {\n",
    "     \"name\": \"office\",\n",
    "     \"prompt\":f\"close up portrait of {token} woman, in the office, sitting, 4K, raw, hrd, hd, high quality, realism, sharp focus\",\n",
    "     \"n_prompt\":\"naked, nsfw, deformed, distorted, disfigured, poorly drawn, bad anatomy, extra limb, missing limb, floating limbs, mutated hands disconnected limbs, mutation, ugly, blurry, amputation\",\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"forest\",\n",
    "     \"prompt\":f\"portrait of {token} woman face, in the forest, with inventory, standing, 4K, raw, hrd, hd, high quality, realism, sharp focus\",\n",
    "     \"n_prompt\":\"naked, nsfw, deformed, distorted, disfigured, poorly drawn, bad anatomy, extra limb, missing limb, floating limbs, mutated hands disconnected limbs, mutation, ugly, blurry, amputation\",\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"street\",\n",
    "     \"prompt\":f\"portrait of smiling {token} woman, on the street, lights, midnight, NY, standing, 4K, raw, hrd, hd, high quality, realism, sharp focus,  beautiful eyes, detailed eyes\",\n",
    "     \"n_prompt\":\"naked, nsfw, deformed, distorted, disfigured, poorly drawn, bad anatomy, extra limb, missing limb, floating limbs, mutated hands, mutation, ugly, blurry\",\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"beach\",\n",
    "     \"prompt\":f\"portrait of {token} woman, on the beach, 4K, raw, hrd, hd, high quality, realism, sharp focus,  beautiful eyes, detailed eyes\",\n",
    "     \"n_prompt\":\"naked, nsfw, deformed, distorted, disfigured, poorly drawn, bad anatomy, extra limb, missing limb, floating limbs, mutated hands, mutation, ugly, blurry\",\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"red_carpet\",\n",
    "     \"prompt\":f\"portrait of smiling {token} woman, on the red carpet, lights, oscar, standing, 4K, raw, hrd, hd, high quality, realism, sharp focus,  beautiful eyes, detailed eyes\",\n",
    "     \"n_prompt\":\"naked, nsfw, deformed, distorted, disfigured, poorly drawn, bad anatomy, extra limb, missing limb, floating limbs, mutated hands, mutation, ugly, blurry\",\n",
    "    },\n",
    "]\n",
    "\n",
    "from PIL import Image\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e2d1914-d84a-44e0-8791-48bb7b857578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d46274205984782aa71c92dd985d232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39dd1ee8dca540378ed0100dc2192014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2c5c0be38b42eb917ed653abd2399a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a1b1cd0267c4dd08f2e72d29dd869ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a99d9fe68e4905bf727cdcba69a4b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa240f3fc1e4b6e93de13baf396326d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318e5e49d367404f92ac010f9b44ad82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe94f787083e49f189b01e3ce8cd01f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffcc4a8f09124c0db902f571b5033e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba8d21fff694511bf838e9a374a960d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = 147525234\n",
    "repeat = 2\n",
    "num_samples = 2\n",
    "guidance_scale = 7.5\n",
    "num_inference_steps = 35\n",
    "height = 768\n",
    "width = 1024\n",
    "\n",
    "save_folder = os.path.abspath(os.path.join(os.pardir, \"jessica_alba\", \"dreambooth_lora_32_report_images\"))\n",
    "save_model = \"with_train_token\"\n",
    "\n",
    "for idx, sample in enumerate(promt_list):\n",
    "  prompt = sample.get(\"prompt\")\n",
    "  negative_prompt = sample.get(\"n_prompt\")\n",
    "  name = sample.get(\"name\")\n",
    "  image_list = []\n",
    "  for _ in range(repeat):\n",
    "    generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
    "    with autocast(\"cuda\"), torch.inference_mode():\n",
    "        images = pipe(\n",
    "            prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            negative_prompt=negative_prompt,\n",
    "            num_images_per_prompt=num_samples,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            generator=generator\n",
    "        ).images\n",
    "    image_list.extend(images)\n",
    "    seed+=345324\n",
    "\n",
    "  img_grid = image_grid(image_list, num_samples, repeat)\n",
    "  save_path = os.path.join(save_folder, save_model, f\"{height}x{width}\")\n",
    "  os.makedirs(save_path, exist_ok=True)\n",
    "  img_grid.save(os.path.join(save_path, f\"{name}.jpg\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
