{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5489e7ed-8138-431c-b561-27fc7bc24a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ildar.azamatov/projects/itmo/itmo-generative-models/jessica_alba/instance_images'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.abspath(os.environ['INSTANCE_DIR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b0a6d49-f7b6-431a-89b3-1b1d3a43326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['INSTANCE_DIR'] = os.path.abspath(os.path.join(os.pardir, \"jessica_alba\", \"instance_images\"))\n",
    "os.environ['CLASS_DIR'] = os.path.abspath(os.path.join(os.pardir, \"jessica_alba\", \"class_images\"))\n",
    "\n",
    "os.environ['MODEL_PATH'] = os.path.abspath(os.path.join(os.pardir, \"models\", \"diffusers_converted_model\"))\n",
    "os.environ['OUTPUT_DIR'] = os.path.abspath(os.path.join(os.pardir, \"models\", \"dreambooth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dde53ead-93e1-449a-9b3a-46ceeddd84bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f05d8262-031e-4472-942b-0270b3ebc72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ildar.azamatov/projects/itmo/itmo-generative-models/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:391: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "04/17/2024 13:42:33 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'variance_type'} was not found in config. Values will be initialized to default values.\n",
      "04/17/2024 13:42:35 - INFO - __main__ - ***** Running training *****\n",
      "04/17/2024 13:42:35 - INFO - __main__ -   Num examples = 500\n",
      "04/17/2024 13:42:35 - INFO - __main__ -   Num batches each epoch = 500\n",
      "04/17/2024 13:42:35 - INFO - __main__ -   Num Epochs = 2\n",
      "04/17/2024 13:42:35 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "04/17/2024 13:42:35 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "04/17/2024 13:42:35 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "04/17/2024 13:42:35 - INFO - __main__ -   Total optimization steps = 800\n",
      "Steps: 100%|████████████| 800/800 [07:21<00:00,  1.80it/s, loss=0.0196, lr=2e-6]04/17/2024 13:49:57 - INFO - accelerate.accelerator - Saving current state to /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/dreambooth/checkpoint-800\n",
      "Configuration saved in /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/dreambooth/checkpoint-800/unet/config.json\n",
      "Model weights saved in /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/dreambooth/checkpoint-800/unet/diffusion_pytorch_model.safetensors\n",
      "04/17/2024 13:50:04 - INFO - accelerate.checkpointing - Optimizer state saved in /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/dreambooth/checkpoint-800/optimizer.bin\n",
      "04/17/2024 13:50:04 - INFO - accelerate.checkpointing - Scheduler state saved in /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/dreambooth/checkpoint-800/scheduler.bin\n",
      "04/17/2024 13:50:04 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/dreambooth/checkpoint-800/sampler.bin\n",
      "04/17/2024 13:50:04 - INFO - accelerate.checkpointing - Random states saved in /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/dreambooth/checkpoint-800/random_states_0.pkl\n",
      "04/17/2024 13:50:04 - INFO - __main__ - Saved state to /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/dreambooth/checkpoint-800\n",
      "Steps: 100%|█████████████| 800/800 [07:29<00:00,  1.80it/s, loss=0.101, lr=2e-6]\n",
      "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded vae as AutoencoderKL from `vae` subfolder of /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/diffusers_converted_model.\n",
      "\n",
      "Loading pipeline components...:  14%|█▊           | 1/7 [00:00<00:00,  6.10it/s]\u001b[A/home/ildar.azamatov/projects/itmo/itmo-generative-models/.venv/lib/python3.10/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Loaded feature_extractor as CLIPFeatureExtractor from `feature_extractor` subfolder of /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/diffusers_converted_model.\n",
      "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/diffusers_converted_model.\n",
      "\n",
      "Loading pipeline components...:  71%|█████████▎   | 5/7 [00:00<00:00, 19.24it/s]\u001b[ALoaded scheduler as DDIMScheduler from `scheduler` subfolder of /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/diffusers_converted_model.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/diffusers_converted_model.\n",
      "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 21.77it/s]\n",
      "Configuration saved in /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/dreambooth/vae/config.json\n",
      "Model weights saved in /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/dreambooth/vae/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/dreambooth/unet/config.json\n",
      "Model weights saved in /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/dreambooth/unet/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/dreambooth/scheduler/scheduler_config.json\n",
      "Configuration saved in /home/ildar.azamatov/projects/itmo/itmo-generative-models/models/dreambooth/model_index.json\n",
      "Steps: 100%|█████████████| 800/800 [07:35<00:00,  1.75it/s, loss=0.101, lr=2e-6]\n"
     ]
    }
   ],
   "source": [
    "!python3 diffusers/examples/dreambooth/train_dreambooth.py \\\n",
    "  --pretrained_model_name_or_path=$MODEL_PATH \\\n",
    "  --instance_data_dir=$INSTANCE_DIR \\\n",
    "  --class_data_dir=$CLASS_DIR \\\n",
    "  --output_dir=$OUTPUT_DIR \\\n",
    "  --instance_prompt=\"a photo of sks woman face\" \\\n",
    "  --class_prompt=\"a photo of woman\" \\\n",
    "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --learning_rate=2e-6 \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --num_class_images=500 \\\n",
    "  --max_train_steps=800 \\\n",
    "  --checkpointing_steps=800 \\\n",
    "  --use_8bit_adam \\\n",
    "  --mixed_precision=\"no\"\\\n",
    "  --train_text_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c428bb44-9d7b-478a-8008-38c10050980f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94bcc3ab4fca4a6ea6ef0fd0574c5bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ildar.azamatov/projects/itmo/itmo-generative-models/.venv/lib/python3.10/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import autocast\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "model_path = os.environ['OUTPUT_DIR']\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_path, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "g_cuda=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7665d0cd-cd02-4d91-94f4-5b877fde678b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1e2f127bd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@markdown Can set random seed here for reproducibility.\n",
    "g_cuda = torch.Generator(device='cuda')\n",
    "seed = 345252 #@param {type:\"number\"}\n",
    "g_cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4949c0a7-d9d4-4c36-b309-870bc16a387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"sks\"\n",
    "promt_list = [\n",
    "    {\n",
    "     \"name\": \"office\",\n",
    "     \"prompt\":f\"close up portrait of {token} woman, in the office, sitting, 4K, raw, hrd, hd, high quality, realism, sharp focus\",\n",
    "     \"n_prompt\":\"naked, nsfw, deformed, distorted, disfigured, poorly drawn, bad anatomy, extra limb, missing limb, floating limbs, mutated hands disconnected limbs, mutation, ugly, blurry, amputation\",\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"forest\",\n",
    "     \"prompt\":f\"portrait of {token} woman face, in the forest, with inventory, standing, 4K, raw, hrd, hd, high quality, realism, sharp focus\",\n",
    "     \"n_prompt\":\"naked, nsfw, deformed, distorted, disfigured, poorly drawn, bad anatomy, extra limb, missing limb, floating limbs, mutated hands disconnected limbs, mutation, ugly, blurry, amputation\",\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"street\",\n",
    "     \"prompt\":f\"portrait of smiling {token} woman, on the street, lights, midnight, NY, standing, 4K, raw, hrd, hd, high quality, realism, sharp focus,  beautiful eyes, detailed eyes\",\n",
    "     \"n_prompt\":\"naked, nsfw, deformed, distorted, disfigured, poorly drawn, bad anatomy, extra limb, missing limb, floating limbs, mutated hands, mutation, ugly, blurry\",\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"beach\",\n",
    "     \"prompt\":f\"portrait of {token} woman, on the beach, 4K, raw, hrd, hd, high quality, realism, sharp focus,  beautiful eyes, detailed eyes\",\n",
    "     \"n_prompt\":\"naked, nsfw, deformed, distorted, disfigured, poorly drawn, bad anatomy, extra limb, missing limb, floating limbs, mutated hands, mutation, ugly, blurry\",\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"red_carpet\",\n",
    "     \"prompt\":f\"portrait of smiling {token} woman, on the red carpet, lights, oscar, standing, 4K, raw, hrd, hd, high quality, realism, sharp focus,  beautiful eyes, detailed eyes\",\n",
    "     \"n_prompt\":\"naked, nsfw, deformed, distorted, disfigured, poorly drawn, bad anatomy, extra limb, missing limb, floating limbs, mutated hands, mutation, ugly, blurry\",\n",
    "    },\n",
    "]\n",
    "\n",
    "from PIL import Image\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4817b6c-bc64-4790-8f74-32a43c329fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c02b242b7e4b8a8fa21a87a8f36751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7d5a09b28f41a3a1f3e6d67bd394ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe7ef599b774ca8aae18e37816e34cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b796fff947477a8097717a5790501c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "844fc6f4d66742ab9253c6096e32770e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f4076aa25241228b1a2d53f0a4c82c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a93046b2e51743988406a2844ab7268f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a46a321878443b6afd90ba392aeb36e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309cb873876e4a908f27cb25c7125f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20cdcbd13ccf4f1bafacace7a92d8589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = 147525234\n",
    "repeat = 2\n",
    "num_samples = 2\n",
    "guidance_scale = 7.5\n",
    "num_inference_steps = 35\n",
    "height = 768\n",
    "width = 1024\n",
    "\n",
    "save_folder = os.path.abspath(os.path.join(os.pardir, \"jessica_alba\", \"dreambooth_report_images\"))\n",
    "save_model = \"with_train_token\"\n",
    "\n",
    "for idx, sample in enumerate(promt_list):\n",
    "  prompt = sample.get(\"prompt\")\n",
    "  negative_prompt = sample.get(\"n_prompt\")\n",
    "  name = sample.get(\"name\")\n",
    "  image_list = []\n",
    "  for _ in range(repeat):\n",
    "    generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
    "    with autocast(\"cuda\"), torch.inference_mode():\n",
    "        images = pipe(\n",
    "            prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            negative_prompt=negative_prompt,\n",
    "            num_images_per_prompt=num_samples,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            generator=generator\n",
    "        ).images\n",
    "    image_list.extend(images)\n",
    "    seed+=345324\n",
    "\n",
    "  img_grid = image_grid(image_list, num_samples, repeat)\n",
    "  save_path = os.path.join(save_folder, save_model, f\"{height}x{width}\")\n",
    "  os.makedirs(save_path, exist_ok=True)\n",
    "  img_grid.save(os.path.join(save_path, f\"{name}.jpg\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
